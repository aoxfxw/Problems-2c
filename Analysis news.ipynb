{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6730d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from IPython.display import IFrame\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from scipy.spatial import distance\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('words')\n",
    "from nltk import stem\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "import ast\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import datasets\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import r_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from IPython.display import IFrame\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from scipy.spatial import distance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "import adjustText\n",
    "from wordcloud import WordCloud\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6e4fa4",
   "metadata": {},
   "source": [
    "# 1.0 Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f91729",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('1news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849dca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822358c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523a74ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = pd.read_excel('1news.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a988eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13322abc",
   "metadata": {},
   "source": [
    "# 3.0 Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083738d1",
   "metadata": {},
   "outputs": [],
   "source": [
    " vectorizer = TfidfVectorizer(input = 'content', strip_accents = 'ascii', \n",
    "                              stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d04977",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ex\n",
    "\n",
    "data = data.drop_duplicates(subset = ['headline']).reset_index(drop = True)\n",
    "\n",
    "data = data.sample(500).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff7f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30823b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [i for i in data['headline']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c785171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into sentences and remove trailing and leading whitespace\n",
    "sentences = []\n",
    "for sentence in text:\n",
    "    sentences.extend(sentence.split('.'))\n",
    "sentences = [s.strip() for s in sentences if s.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf0914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for i in sentences:\n",
    "    text.append(i.lower())\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    if isinstance(sentences[i], str):\n",
    "        sentences[i] = sentences[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c3b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1059dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectors.todense().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17291d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = pd.DataFrame(\n",
    "    vectors,columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ddb615",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f7778",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_1 = PCA(n_components = 3)\n",
    "comps_1 = pca_1.fit_transform(ff)\n",
    "pc_ff_1 = pd.DataFrame(data = comps_1, columns = ['PC'+str(i) for i in range(1, comps_1.shape[1]+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6434d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_ff_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdb996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward').fit(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f5686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=10).fit(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fa6c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward').fit(pc_ff_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=10).fit(pc_ff_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eff692",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_all = pd.concat([ff, pc_ff_1], axis = 1)\n",
    "ff_all['clusters_ag'] = [str(i) for i in clustering.labels_]\n",
    "ff_all['clusters_knn'] = [str(i) for i in kmeans.labels_]\n",
    "ff_all['headline'] = data['headline']\n",
    "ff_all['topic'] = data['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a046da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(ff_all, x='PC1', y='PC2', z='PC3',\n",
    "              color='clusters_ag', hover_data = ['headline'])\n",
    "\n",
    "fig.update_traces(marker=dict(size = 5, line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "\n",
    "fig.show('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa2aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e747428",
   "metadata": {},
   "source": [
    "# 4.0 Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb4879",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "entities = []\n",
    "labels = []\n",
    "position_start = []\n",
    "position_end = []\n",
    "\n",
    "for item in df['headline']: \n",
    "    doc = nlp(str(item)) \n",
    "    for ent in doc.ents:\n",
    "        entities.append(ent)\n",
    "        labels.append(ent.label_)\n",
    "        position_start.append(ent.start_char)\n",
    "        position_end.append(ent.end_char)\n",
    "df_ner = pd.DataFrame({'Entities':entities, 'Labels':labels,'Position_Start': position_start, 'Position_end':position_end})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ca0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3c1c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner['headline'] = df['headline']\n",
    "label_counts = df_ner['Labels'].value_counts()\n",
    "entity_freq = df_ner.groupby('Labels').size().reset_index(name='count')\n",
    "# sort the entities by frequency in descending order\n",
    "entity_freq = entity_freq.sort_values(by=['count'], ascending=False)\n",
    "df_ner['headline'] = df['headline']\n",
    "label_counts = df_ner['Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dadf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4d0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['ORG', 'DATE', 'NORP', 'PERSON', 'GPE', 'ORDINAL', 'CARDINAL', 'MONEY', 'LANGUAGE', 'TIME', 'QUANTITY', 'PERCENT']\n",
    "counts = [536, 238, 104, 44, 37, 22, 15, 13, 5, 4, 1, 1]\n",
    "plt.bar(labels, counts)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('NER Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of NER Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e6f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save figure as PNG\n",
    "fig.write_image(\"bargraphNER.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c524e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d8f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = sum(counts)\n",
    "\n",
    "entity_freq = {'Labels': labels, 'count': counts}\n",
    "percentages = [(count / total_count) * 100 for count in counts]\n",
    "\n",
    "# Convert percentages to one-dimensional array\n",
    "percentages = np.ravel(percentages)\n",
    "\n",
    "# Create pie chart\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.pie(percentages, labels=labels, autopct='%1.1f%%')\n",
    "plt.title('Percentage Distribution of NER Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095fd9ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad261bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ba7b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae461641",
   "metadata": {},
   "source": [
    "# 5.0 Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bef8fb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf945ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into sentences and remove trailing and leading whitespace\n",
    "text = []\n",
    "for sentence in df['headline']:\n",
    "    text.extend(sentence.split('.'))\n",
    "text = [s.strip() for s in text if s.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1c2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse the list of strings into a single long string for processing\n",
    "text = ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7c85c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove forward slashes (\"/\")\n",
    "text = text.replace('/', '')\n",
    "\n",
    "# Remove single quotes (\"'\")\n",
    "text = text.replace(\"'\", '')\n",
    "\n",
    "# Remove square brackets (\"'\")\n",
    "text = text.replace(\"[\", '')\n",
    "\n",
    "# Remove square brackets (\"'\")\n",
    "text = text.replace(\"]\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a709ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fde9a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88648d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(text)\n",
    "# converts the words in word_tokens to lower case and then checks whether\n",
    "#they are present in stop_words or not\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "#with no lower case conversion\n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7598c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dbfde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove commas (\",\") from the list of strings\n",
    "stops = [s.replace(',', '') for s in filtered_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b7f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty strings from the list\n",
    "stops = [s for s in stops if s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00431223",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a string\n",
    "string = ' '.join(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c3728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a wordcloud\n",
    "word_cloud = WordCloud(collocations = False, background_color = 'white').generate(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the wordcloud\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e8405d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e544afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d50fde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
